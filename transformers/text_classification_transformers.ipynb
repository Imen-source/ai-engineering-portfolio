{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1-ydeW8-InB"
   },
   "outputs": [],
   "source": [
    "# üìå Notes & Clarifications\n",
    "\n",
    "1Ô∏è‚É£ **Model Warnings**\n",
    "- Some weights of `DistilBertForSequenceClassification` are **newly initialized** because we added a classification head for the IMDb dataset.\n",
    "- This is normal and expected. The model needs to be trained on the downstream task before making reliable predictions.\n",
    "\n",
    "2Ô∏è‚É£ **Hugging Face Hub Token**\n",
    "- A warning may appear if no `HF_TOKEN` is set in the environment.\n",
    "- Public models and datasets work without authentication, so this is **non-blocking**.\n",
    "\n",
    "3Ô∏è‚É£ **CPU/GPU & Triton**\n",
    "- On CPU-only environments, you may see warnings about **Triton** or pinned memory.\n",
    "- These are just optimizations for GPU; they do **not affect correctness**.\n",
    "\n",
    "4Ô∏è‚É£ **Trainer FutureWarnings**\n",
    "- The `tokenizer` argument in `Trainer` is deprecated and will be removed in future versions.\n",
    "- The notebook is fully functional; this is just for **future-proofing**.\n",
    "\n",
    "5Ô∏è‚É£ **Evaluation Results**\n",
    "- The accuracy shown is from a **small subset for demo purposes**.\n",
    "- Training on the full dataset for multiple epochs will improve performance.\n",
    "\n",
    "6Ô∏è‚É£ **W&B (Weights & Biases)**\n",
    "- You can choose whether to track metrics online or offline.\n",
    "- In this notebook, metrics are tracked **locally offline**, which is enough for demonstration.\n",
    "\n",
    "7Ô∏è‚É£ **Takeaways**\n",
    "- This notebook demonstrates the **full workflow**: loading a dataset, tokenizing, setting up a DistilBERT model, training, and evaluating.\n",
    "- Warnings are mostly informational and do **not indicate errors**.\n",
    "- It‚Äôs ready for **portfolio purposes** to show knowledge of Transformers, tokenization, training, evaluation, and handling real NLP data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tenk9l64FVf"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets evaluate -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcYVqe_45a-o",
    "outputId": "0480b63b-dc47-4bb3-9e8d-24e3b2e57262"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695,
     "referenced_widgets": [
      "25a66e7b6f764512be50599e0c3aea9d",
      "6775a61280cb4c58af70b5585c265676",
      "62d9a6bb3acc4f4f81fae6daeb9f15f5",
      "92b158e4c81e404b9c34644934ab6b20",
      "4ed452c954d144d6833ad0a960b45a42",
      "25e719b0d5e74845a73fa43543e56fba",
      "e58af2be661541518713142758a9f1ab",
      "5e6a14b1642542d4be58b17c75964870",
      "2c188d5490f24208b007c5dab0d08a71",
      "3c53c2e9887f452fbc6888f86fe63584",
      "b5c7478be5f94f98927ee5aece4d4acf",
      "e58352b955b34115a660947b1f776fc0",
      "ed52bbed95d746fca687a4ba65d84689",
      "674bd1e550824c74b66604ae359a5449",
      "1c8d7e4415044cb4a684948faf1c0c06",
      "124f27adfb77469b8ea6ac8154b2f4a8",
      "066249deb8d743d8a8ec82c830d8003b",
      "e2cec2c5a41e43be9a3ed7988a73841a",
      "e81d3783fa634c26b9b6dace891b7589",
      "477a5e830f41410abdd36aafe020eda3",
      "e3545a462d834bc790977123734fe373",
      "a870734606ef49ceb4bb80bbb305418f",
      "453124e9251c485aaf7f151e724594f5",
      "eba84a0c03e548ec9f7245a95e77a385",
      "785209fe531b45d786dad40732c45e4b",
      "e5c4a09ec79b405eb6843a3c7e594ad4",
      "f60a80d02e584c9ba67ec2b036cc8a1c",
      "0eed54dd15d345a8bdcc5a4e3e41e639",
      "c3721236ce084d179eb2c1b9c75b4ee0",
      "69278fb071cb421da8e7876947cdeacd",
      "50c746e61ff24c3c9b29ab72a4cf0d54",
      "95e2ffaf56814d4ab454474b5d8e2d5a",
      "939e4c80698d4945b70a0a6c50f6973f",
      "5d0f556cdb5a46dc9d6fdd6d295d1b81",
      "bdf1f4bbdc5f457ab3a0e66e02ba9273",
      "6436d6de4c264e9c9f28888a0c35a748",
      "410243628eb444f18e64510e35bc5cbc",
      "431ef1cfdf774994a339467c2f65a49e",
      "3e189cf8eec647509d847eacfbfbd2e4",
      "2b704060bf1042d2b9cc3ccf7b88db9f",
      "072ec39ab7dd4ab09f7cac7857c3623c",
      "b905acd93c324ba7abd49cfe55c1296b",
      "d114bc63af62471fb09676986350c9aa",
      "dd1a6823f2904175a03773aa20fd50e0"
     ]
    },
    "id": "RlYCh8Mw0reV",
    "outputId": "564a9836-50e7-4e03-e107-763f750f7ad6"
   },
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Install / upgrade packages if needed\n",
    "\n",
    "# 2Ô∏è‚É£ Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "# 3Ô∏è‚É£ Load dataset (IMDb sentiment)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# 4Ô∏è‚É£ Initialize tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# 5Ô∏è‚É£ Tokenize dataset\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 6Ô∏è‚É£ Set training arguments (compatible with older transformers)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    ")\n",
    "\n",
    "# 7Ô∏è‚É£ Prepare metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 8Ô∏è‚É£ Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle(seed=42).select(range(200)),  # small subset for demo\n",
    "    eval_dataset=tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200)),    # small subset for demo\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 9Ô∏è‚É£ Train & evaluate\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
