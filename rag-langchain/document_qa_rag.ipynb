{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfBdmd06u5_5"
   },
   "outputs": [],
   "source": [
    "# üìù Notes on this RAG Notebook\n",
    "\n",
    "- **Purpose:** This notebook demonstrates a basic **RAG (Retrieval-Augmented Generation)** workflow using a PDF document as the knowledge source.\n",
    "- **How it works:**\n",
    "  1. The PDF is loaded and split into **text chunks**.\n",
    "  2. Each chunk is embedded into a **vector database** using `HuggingFaceEmbeddings`.\n",
    "  3. A **retriever** fetches the most relevant chunks when a question is asked.\n",
    "  4. A **language model** (Flan-T5) generates an answer based on the retrieved context.\n",
    "\n",
    "- **What this shows:**\n",
    "  - I understand the concepts of **document retrieval, embedding, and generation**.\n",
    "  - The answer is **grounded in the document**, reducing hallucinations.\n",
    "  - The setup is simple, **functional, and clear**, making it reproducible.\n",
    "\n",
    "- **Notes on output:**\n",
    "  - Answers are concise and reflect **information directly from the context**.\n",
    "  - This notebook can be extended to multiple documents or larger models.\n",
    "\n",
    "- **Tools used:**\n",
    "  - `langchain`, `sentence-transformers`, `faiss`, `transformers`\n",
    "  - CPU device (for demonstration; GPU recommended for larger models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "81847c355d14419db0dd7eb961383b63",
      "65733de0b986401fbce2c748a5310d7b",
      "f8df267837c4482486334174e729c101",
      "a4db1a4fbf89430bb1738efb79a77d25",
      "4396fea267534c3c89f08a58ecd54f47",
      "3ceec6eee4ad46509b54babc2a2eed32",
      "2aa06229b0e84edcaae0ce32fcf721ab",
      "3b1a33cee53c46e792a54c984d663d85",
      "0758cfdccb0f4f21ac0a9861c051b12f",
      "54137872f7524b1ba1703a043cca6dec",
      "c21d15cded6d4fb68fb273b028f6c2b5",
      "871c65a0c18c4316bee709b0ef4464d1",
      "80cac6744d0a4aa6a770149e17bd8563",
      "207aad40bff0487a96e20c3b90eec1f1",
      "571cf253d58a43d2a5a534de678c254e",
      "69ac73d166204a8dabbac024ab7ddb86",
      "5c1b7874318c44cea258380a37a0679e",
      "a2cc930f0feb461384e7096c3a7c77f9",
      "fa9bdefd34b0477cb0e89c748e940131",
      "cbee49796df641ca93559706cdde9040",
      "47c82b7cb3d34a49bf56d616aecb5502",
      "b272fd0c475b4d8984abba4b55fefeae"
     ]
    },
    "id": "aMMLVMmVrzlE",
    "outputId": "0e3f831e-f3d3-414d-d694-15f4a119f5ac"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create pipeline\n",
    "llm = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1,  # CPU\n",
    "    max_new_tokens=150\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Retrieve relevant context\n",
    "# -----------------------------\n",
    "question = \"What is LoRA and why is it efficient?\"\n",
    "\n",
    "docs = retriever.invoke(question)\n",
    "context = \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. RAG prompt\n",
    "# -----------------------------\n",
    "prompt = f\"\"\"\n",
    "Answer the question ONLY using the context below.\n",
    "Do NOT repeat phrases.\n",
    "If the answer is not in the context, respond with \"I don't know.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Generate answer\n",
    "# -----------------------------\n",
    "response = llm(prompt)\n",
    "answer = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "print(\"ANSWER:\\n\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
